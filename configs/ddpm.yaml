# SPECIFY RUN-NAME
run_name: "SK-ddpm_128-Linear-07Dec"

# Sampling / scheduler
model_type: "unet"
use_ddim: false         # <--- CRITICAL: Set to false for DDPM
ddim_eta: 0.0           # Kept for compatibility, though ignored in standard DDPM
clip_sample: true
clip_sample_range: 1.0

# Paths
output_dir: experiments
# Using the common dataset folder (Copied from your working file)
data_dir: /jet/home/jgupta2/hw5_student_starter_code/data/imagenet100_128x128/train
val_dir:  /jet/home/jgupta2/hw5_student_starter_code/data/imagenet100_128x128/validation

# Data / training
seed: 42
image_size: 128
batch_size: 32          # Kept at 32 to ensure it fits in GPU memory (like your working run)
num_workers: 0          # <--- CRITICAL: Set to 0 to prevent the DataLoader crash seen with num_workers:2
num_classes: 100
num_epochs: 200         # Increased to 200 to match your DDIM run for fair comparison
patience: 100           # Added for early stopping compatibility
learning_rate: 1.0e-4   # Standard DDPM often uses 1e-4 or 2e-4
weight_decay: 1.0e-5    # Aligned with your working file

# Diffusion schedule
num_train_timesteps: 1000
num_inference_steps: 1000 # <--- CRITICAL: For DDPM, this MUST equal train_timesteps; cannot speed this up without DDIM.
beta_start: 1.0e-4
beta_end: 0.02
beta_schedule: linear   # Linear is the classic DDPM schedule (Ho et al., 2020)

variance_type: fixed_small
prediction_type: epsilon

# UNet Architecture 
unet_in_size: 128 
unet_in_ch: 3
unet_ch: 128            
unet_num_res_blocks: 2
unet_ch_mult: [1, 2, 2, 4]
unet_attn: [2, 3]
unet_dropout: 0.1  # Added dropout for consistency

# Extra features (Explicitly disabled)
latent_ddpm: false
use_cfg: false          # Standard DDPM usually doesn't use CFG, but can enable it if and when needed for implementation for DDPM.
cfg_guidance_scale: 1.0 # Set to 1.0 (no guidance) if use_cfg is false

# FID / IS evaluation
eval_fid_is: true
eval_samples: 1000 # 5000 for inference.
eval_frequency: 5